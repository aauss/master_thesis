\chapter{Results and Evaluation}

\section{EDB Analysis}\label{edb analysis}
A substantial part of my work was the exploiting of available sources which includes the EDB and scraped epidemiological news. In the following I introduce how I managed to recover most of the information in the EDB for my further analyzes and how I managed to scrape a large amount epidemiological data with a limited time profile.

\subsection{Source Determination}
Before I could train a classifier to detect relevant articles or extract meaningful key words from texts, I needed to create a labeled dataset from the EDB. Therefor, I collected all articles that were read by the epidemiologists but were not put into the EDB. However, writing an algorithm for information retrieval can be time consuming so I needed to narrow down the, at that moment, 75 sources used in the EDB.

The decision which source to extract was then made by the relevance of the source, the complexity to retrieve information from this source, and the relevance of the content provided by this source.
First, I extracted all URLs from the EDB and clustered them based on their netloc to rank the sources used in the EDB (Fig. \ref{fig:netloc}). Then, I focused only on the most used sources that were most accessible (Fig. \ref{table:INIGsources}). The difficulty to access the information of the text, on the other hand, I found by exemplary text extractions. Additionally, I made an evaluation that showed how relevant the source is for the work of INIG. All sources that are mandatory for INIG to observe are shown in Tab. \ref{table:INIGsources}.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.8]{netloc.pdf}
    \caption{The netflow frequency of sources used in the EDB. Shown in grey is the sum of EDB entries referencing a source with a netloc not shown in this figure.}
    \label{fig:netloc}
\end{figure}


\begin{table}[h!]
  \centering
  \begin{tabular}{@{}cccc@{}}
    \toprule
    \textbf{Source} & \textbf{Data Format} & \textbf{Data Quality} & \textbf{Accesibility}\\
    \midrule
    \href{http://www.cidrap.umn.edu/}{CIDRAP} & HTML & Mixed content & Intermediate \\
    \href{http://www.promedmail.org/}{ProMED Mail} & HTML & Only relevant & Easy\\
    \href{http://www.who.int/csr/don/en/}{WHO DONs} & HTML & Only relevant & Easy \\
    EIOS daily digest & Email & Only relevant & Hard \\
    \href{http://outbreaknewstoday.com/}{OutbreakNewsToday} & HTML & Mixed content & Intermediate\\
    ECDC Report & Email & Only relevant & Hard \\
    \href{http://www.afro.who.int/fr/health-topics/disease-outbreaks/outbreaks-and-other-emergencies-updates}{WHO Afro Bulletin} & PDF & Only relevant & Hard \\
    \href{http://www.eurosurveillance.org/content/eurosurveillance/browse}{EuroSurveillance} & PDF and HTML & Mixed content & Intermediate \\
    \href{http://www.who.int/wer/en/}{WHO WER} & PDF & Mixed content & Hard\\
    \href{https://ecdc.europa.eu/en/threats-and-outbreaks/reports-and-data/weekly-threats}{ECDC CDTR} & PDF & Only relevant & Intermediate \\
    \href{http://www.emro.who.int/pandemic-epidemic-diseases/information-resources/weekly-epidemiological-monitor.html}{WHO EMRO} & PDF & Mixed content & Hard \\
    \href{https://www.paho.org/hq/index.php?option=com_content&view=article&id=14044:epidemiological-alerts-archive-by-year-2018&Itemid=72203&lang=en}{WHP PAHO} & PDF & Only relevant & Intermediate\\
    \bottomrule
  \end{tabular}
  \caption{A list of the obligated sources for the daily screening of INIG and the data type of the source. Furthermore, I assessed the sources for their quality, i.e., whether they only contain information relevant for epidemiological surveillance or also research findings and ongoing projects (mixed content). After exemplary testing, I rated the difficulty to extract analyzable text from these sources where \textsl{easy} posed no difficulty, \textsl{intermediate} would have required additional work but is promising to function and \textsl{hard} was unsure whether it could work satisfactorily.}
  % \setlength{\tabcolsep}{1.5em}
\label{table:INIGsources}
\end{table}

\subsection{Data Quality}
Due to the unrestrained column settings, every entry in the EDB was free text with spelling mistakes and different formatting. The transferral of the EDB into a controlled vocabulary was a vital steps since it made more data points useable. Tab. \ref{table:preprocessing performance} shows that the preprocessing made up to 100 EDB entries accessible per keyword.

\begin{table}
  \centering
  \caption{Performance evaluation of the EDB preprocessing per key word. The table shows the amount of valid and invalid entries before and after preprocessing was applied. The EDB in sum has 557 entries.  }
  \begin{tabular}{@{}cccc@{}}
    \toprule
    \textbf{Key Word} & \textbf{Valid Before} & \textbf{Valid After} & \textbf{Invalid After} \\
    & \textbf{Preprocessing} & \textbf{Preprocessing} & \textbf{Preprocessing} \\
    \midrule
    Date & 168 & 168 &  19 \\
    Case count & 299 & 394 &  18 \\
    Country & 355 & 494 &  17 \\
    Disease & 231 & 332 & 16 \\
    \bottomrule
  \end{tabular}
  \label{table:preprocessing performance}
\end{table}

\section{Key Information Extraction}
Although, EpiTator extracted only relevant entity classes, I still needed to filter them.
To do so, I used the most often occurring entity in one entity class e.g., the most frequent disease in one text (Lis. \ref{lst:mostoccure}).
Additionally, I trained NBCs using all sentences of a text containing a specific entity class and used the extracted key entity from the EDB to detect the sentence matching the EDB entry to then label the data.

\section{Article Recommendation}
For the detection of relevant disease outbreak articles, I used the scraped WHO DONs and ProMED Mail articles of 2018 together with the EDB entries from both those sources as the training data.

\section{Web App}
